"""
Service RAG (Retrieval-Augmented Generation)

Architecture :
Question ‚Üí Embedding ‚Üí ChromaDB ‚Üí Top K chunks ‚Üí Prompt + LLM ‚Üí R√©ponse
"""

import os
from pathlib import Path
from typing import List, Dict, Optional
import warnings
warnings.filterwarnings('ignore')

from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from huggingface_hub import InferenceClient
from dotenv import load_dotenv

load_dotenv()

# ============================================
# CONFIGURATION
# ============================================

CHROMA_DB_PATH = "data/chroma_db"
COLLECTION_NAME = "student_documents"
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
LLM_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")

# ============================================
# SINGLETON : Mod√®le d'embeddings global
# ============================================

_embedding_model = None  # Variable globale

def get_embedding_model():
    """
    Retourne le mod√®le d'embeddings (charg√© une seule fois)
    Pattern Singleton
    """
    global _embedding_model
    
    if _embedding_model is None:
        print("üì¶ Chargement mod√®le embeddings (une seule fois)...")
        
        _embedding_model = SentenceTransformer(
            EMBEDDING_MODEL,
            device='cpu',
            cache_folder=os.path.expanduser("~/.cache/huggingface/")
        )
        
        print("‚úÖ Mod√®le embeddings charg√© et pr√™t")
    
    return _embedding_model

# ============================================
# SERVICE RAG
# ============================================

class RAGService:
    """
    Service principal pour le RAG
    
    Responsabilit√©s :
    1. Recherche de documents pertinents (retrieval)
    2. G√©n√©ration de r√©ponse avec contexte (generation)
    3. Citation des sources
    """
    
    def __init__(self):
        """Initialise le service RAG (r√©utilise le mod√®le global)"""
        
        print("üöÄ Initialisation du service RAG...")
        
        # 1. Embeddings (r√©utilise mod√®le global)
        self.embedding_model = get_embedding_model()  # ‚Üê Utilise singleton
        
        # 2. ChromaDB
        print(f"   üóÑÔ∏è Connexion ChromaDB...")
        self.chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
        self.collection = self.chroma_client.get_collection(name=COLLECTION_NAME)
        
        # 3. LLM
        print(f"   ü§ñ Connexion HuggingFace API...")
        self.llm_client = InferenceClient(token=HUGGINGFACE_API_KEY)
        
        # 4. M√©moire
        self.conversation_history = []
        self.max_history = 5
        
        print("‚úÖ Service RAG pr√™t !\n")
    

    def add_to_history(self, question: str, answer: str):
        """
        Ajoute un √©change √† l'historique
        
        Args:
            question: Question de l'utilisateur
            answer: R√©ponse de l'assistant
        """
        
        self.conversation_history.append({
            'question': question,
            'answer': answer
        })
        
        # Limiter la taille de l'historique
        if len(self.conversation_history) > self.max_history:
            self.conversation_history.pop(0)
    
    
    def get_conversation_context(self) -> str:
        """
        Construit le contexte conversationnel
        
        Returns:
            Historique format√© pour le prompt
        """
        
        if not self.conversation_history:
            return ""
        
        context = "\nHistorique de la conversation :\n"
        for i, exchange in enumerate(self.conversation_history, 1):
            context += f"\n√âchange {i}:\n"
            context += f"√âtudiant: {exchange['question']}\n"
            context += f"Assistant: {exchange['answer']}\n"
        
        return context
    
    
    def clear_history(self):
        """Efface l'historique de conversation"""
        self.conversation_history = []
        print("üóëÔ∏è Historique effac√©")
    
    
    def retrieve_documents(
    self, 
    query: str, 
    n_results: int = 3,
    category_filter: Optional[str] = None
) -> List[Dict]:
        """
        Recherche les documents pertinents
        
        AM√âLIOR√â : D√©tection automatique de cat√©gorie
        """
        
        # D√©tecter automatiquement la cat√©gorie si non fournie
        if not category_filter:
            category_filter = self._detect_category(query)
        
        # G√©n√©rer embedding de la question
        query_embedding = self.embedding_model.encode([query])[0]
        
        # Pr√©parer filtres ChromaDB
        where_filter = None
        if category_filter and category_filter != "all":
            where_filter = {"category": category_filter}
        
        # Rechercher dans ChromaDB
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=n_results * 2,  # R√©cup√©rer plus pour filtrer
                where=where_filter
            )
            
            # Formater les r√©sultats
            documents = []
            for doc, meta, distance in zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            ):
                documents.append({
                    'text': doc,
                    'metadata': meta,
                    'score': 1 / (1 + abs(distance))
                })
            
            # Limiter au nombre demand√©
            documents = documents[:n_results]
            
            return documents
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur retrieval : {e}")
            # Fallback sans filtre
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=n_results
            )
            
            documents = []
            for doc, meta, distance in zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            ):
                documents.append({
                    'text': doc,
                    'metadata': meta,
                    'score': 1 / (1 + abs(distance))
                })
            
            return documents


    def _detect_category(self, query: str) -> Optional[str]:
        """
        D√©tecte automatiquement la cat√©gorie d'une question
        
        Args:
            query: Question de l'utilisateur
        
        Returns:
            Cat√©gorie d√©tect√©e ou None
        """
        
        query_lower = query.lower()
        
        # Mots-cl√©s par cat√©gorie
        categories = {
            'emploi_temps': [
                'emploi du temps', 'calendrier', 'horaire', 'planning',
                'quand commence', 'd√©but', 'fin semestre', 'vacances',
                'cours', 's√©ance', 'date examen', 'rentr√©e'
            ],
            'reglements': [
                'r√®glement', 'r√®gle', 'charte', 'interdit', 'autoris√©',
                'absence', 'retard', 'sanction', 'discipline',
                'droit', 'obligation', 'infraction'
            ],
            'procedures': [
                'inscription', 'comment', 'proc√©dure', 'd√©marche',
                'documents', 'dossier', 'attestation', 'certificat',
                's\'inscrire', 'demande', 'formulaire'
            ],
            'faqs': [
                'faq', 'question fr√©quente', 'aide', 'information',
                'contact', 'o√π trouver', 'qui contacter'
            ]
        }
        
        # Compter matches par cat√©gorie
        scores = {}
        for category, keywords in categories.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > 0:
                scores[category] = score
        
        # Retourner cat√©gorie avec le plus de matches
        if scores:
            best_category = max(scores, key=scores.get)
            print(f"   üè∑Ô∏è Cat√©gorie d√©tect√©e : {best_category}")
            return best_category
        
        return None
    
    
    def generate_prompt(
    self, 
    query: str, 
    documents: List[Dict],
    include_history: bool = True
) -> str:
        """
        Prompt RENFORC√â contre hallucinations
        """
        
        # Contexte documentaire
        context = ""
        for i, doc in enumerate(documents, 1):
            source = doc['metadata']['source']
            text = doc['text']
            
            context += f"\n[Document {i} - {source}]\n{text}\n"
            context += "-" * 60 + "\n"
        
        # Historique
        history_context = ""
        if include_history and self.conversation_history:
            history_context = "\n\n√âchanges pr√©c√©dents :\n"
            for i, exchange in enumerate(self.conversation_history[-3:], 1):
                history_context += f"Q{i}: {exchange['question']}\n"
                history_context += f"R{i}: {exchange['answer'][:100]}...\n\n"
        
        # Prompt renforc√©
        prompt = f"""Tu es un assistant de l'UM5. Tu dois √™tre TR√àS PRUDENT et NE JAMAIS inventer d'informations.

    CONTEXTE :
    Tu as acc√®s √† des documents officiels limit√©s.
    {history_context}

    DOCUMENTS DISPONIBLES :
    {context}

    ‚ö†Ô∏è R√àGLES CRITIQUES - √Ä RESPECTER ABSOLUMENT :

    1. SALUTATIONS (bonjour, merci, ok, au revoir) :
    ‚Üí R√©ponds poliment SANS utiliser les documents

    2. QUESTIONS N√âCESSITANT RECHERCHE :
    ‚Üí Utilise UNIQUEMENT les informations EXPLICITES dans les documents ci-dessus
    ‚Üí Si l'information N'EST PAS EXPLICITEMENT dans les documents, tu DOIS dire :
        "Je n'ai pas cette information dans ma base de connaissances. Je vous conseille de contacter [service concern√©]."
    
    3. NE JAMAIS :
    ‚ùå Inventer des URLs, emails, num√©ros de t√©l√©phone
    ‚ùå Inventer des proc√©dures non mentionn√©es
    ‚ùå Extrapoler ou d√©duire des informations
    ‚ùå Donner des infos g√©n√©rales si la question est sp√©cifique
    
    4. STYLE :
    ‚úÖ Concis (2-3 phrases max)
    ‚úÖ Citer la source : "Selon [Document X]..."
    ‚úÖ Si incomplet : "Les documents ne pr√©cisent pas... Je vous conseille de..."

    QUESTION :
    {query}

    R√âPONSE (prudente, pr√©cise, cit√©e) :
    
    CONTACTS UTILES (√† mentionner si info manquante) :
    - Service scolarit√© de votre facult√©
    - Plateforme de pr√©inscription : https://preinscription.um5.ac.ma
    - Site officiel UM5 : https://www.um5.ac.ma"""

        return prompt
        
    
    def generate_answer(
        self, 
        prompt: str, 
        max_tokens: int = 500,
        temperature: float = 0.7
    ) -> str:
        """
        G√©n√®re la r√©ponse avec le LLM
        
        Args:
            prompt: Prompt complet avec contexte
            max_tokens: Longueur max de la r√©ponse
            temperature: Cr√©ativit√© (0=d√©terministe, 1=cr√©atif)
        
        Returns:
            R√©ponse g√©n√©r√©e par le LLM
        """
        
        try:
            # Appel API HuggingFace
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            response = self.llm_client.chat_completion(
                messages=messages,
                model=LLM_MODEL,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            
            answer = response.choices[0].message.content
            return answer.strip()
            
        except Exception as e:
            return f"Erreur lors de la g√©n√©ration : {str(e)}"
    
    def reformulate_query(self, query: str) -> str:
        """
        Reformule la question en incluant le contexte conversationnel
        Exemple :
        Historique: "Quand commence le semestre d'automne ?"
        Question: "Et combien de temps dure-t-il ?"
        Reformul√©: "Combien de temps dure le semestre d'automne ?"
        Args:
        query: Question actuelle (peut √™tre vague)
        Returns:
        Question reformul√©e (plus explicite)
        """
    
        # Si pas d'historique ou question d√©j√† explicite, retourner tel quel
        if not self.conversation_history:
            return query
    
        # Si question courte avec pronom (il, elle, √ßa, etc.)
        pronouns = ['il', 'elle', '√ßa', 'cela', 'ils', 'elles']
        is_followup = any(pronoun in query.lower() for pronoun in pronouns)
        if not is_followup and len(query.split()) > 5:
            return query  # Question d√©j√† explicite
    
        print(f"   üîÑ Reformulation avec contexte...")
    
        # Construire prompt de reformulation
        last_exchange = self.conversation_history[-1]
        reformulation_prompt = f"""Tu dois reformuler une question de suivi pour la rendre explicite.
        Contexte de la conversation pr√©c√©dente :
        Question pr√©c√©dente : {last_exchange['question']}
        R√©ponse donn√©e : {last_exchange['answer'][:200]}
        Question de suivi (vague) : {query}
        Ta t√¢che : Reformuler cette question de suivi pour qu'elle soit explicite et autonome.
        Ne r√©ponds PAS √† la question, reformule-la seulement.
        Exemple :
        Contexte : "Quand commence le semestre d'automne ?"
        Question : "Et combien de temps dure-t-il ?"
        Reformul√© : "Combien de temps dure le semestre d'automne ?"

        Reformulation (une seule phrase, sans explication) :"""

        try:
            messages = [{"role": "user", "content": reformulation_prompt}]
        
            response = self.llm_client.chat_completion(
                messages=messages,
                model=LLM_MODEL,
                max_tokens=50,
                temperature=0.3  # Bas pour √™tre pr√©cis
            )
        
            reformulated = response.choices[0].message.content.strip()
            print(f"   ‚úÖ Reformul√© : {reformulated}")
        
            return reformulated
        
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur reformulation, utilisation question originale")
            return query
        

    
    def is_greeting(self, question: str) -> bool:
        """
        D√©tecte si c'est une salutation OU formule de politesse
        (ne n√©cessitant pas de recherche documentaire)
        """
        
        # Salutations et formules de politesse
        simple_phrases = [
            # Salutations
            'bonjour', 'salut', 'hello', 'hi', 'hey', 'coucou', 'bonsoir',
            # Politesse
            'merci', 'thanks', 'thank you', 'd\'accord', 'ok', 'okay',
            'au revoir', 'bye', '√† bient√¥t', '√† plus',
            # Expressions courtes
            'oui', 'non', 'bien', 'super', 'cool', 'parfait', 'g√©nial'
        ]
        
        question_lower = question.lower().strip()
        
        # Phrase courte (1-3 mots)
        if len(question_lower.split()) <= 3:
            return any(phrase in question_lower for phrase in simple_phrases)
        
        return False


    def handle_greeting(self, question: str) -> str:
        """
        R√©ponse adapt√©e selon le type de message
        """
        
        question_lower = question.lower().strip()
        
        # Merci / Remerciements
        if any(word in question_lower for word in ['merci', 'thanks', 'thank you']):
            responses = [
                "De rien ! üòä N'h√©sitez pas si vous avez d'autres questions.",
                "Avec plaisir ! Je suis l√† pour vous aider.",
                "Heureux de vous aider ! Autre chose ?"
            ]
        
        # Au revoir
        elif any(word in question_lower for word in ['au revoir', 'bye', '√† bient√¥t', '√† plus']):
            responses = [
                "√Ä bient√¥t ! Bonne journ√©e ! üëã",
                "Au revoir ! N'h√©sitez pas √† revenir si besoin.",
                "√Ä plus tard ! Bonne continuation dans vos √©tudes ! üéì"
            ]
        
        # Confirmations (ok, d'accord, etc.)
        elif any(word in question_lower for word in ['ok', 'okay', 'd\'accord', 'bien', 'parfait']):
            responses = [
                "Super ! Autre question ?",
                "Parfait ! Comment puis-je vous aider d'autre ?",
                "D'accord ! N'h√©sitez pas pour d'autres questions."
            ]
        
        # Salutations par d√©faut
        else:
            responses = [
                "Bonjour ! üëã Je suis l'assistant virtuel de l'UM5. Comment puis-je vous aider ?",
                "Salut ! üòä Posez-moi vos questions sur les emplois du temps, r√®glements et proc√©dures.",
                "Bonjour ! Bienvenue ! Que souhaitez-vous savoir sur l'UM5 ?"
            ]
        
        import random
        return random.choice(responses)
    
    
    def ask(
        self, 
        question: str,
        n_results: int = 3,
        use_history: bool = True
    ) -> Dict:
        """
        Pipeline RAG complet avec gestion salutations
        """
        
        print(f"\n‚ùì Question : {question}")
        
        # 0. D√©tecter salutation
        if self.is_greeting(question):
            print("   üëã Message simple d√©tect√© (pas de recherche doc)")
            
            answer = self.handle_greeting(question)  # ‚Üê Passer la question
            
            return {
                'question': question,
                'answer': answer,
                'sources': [],
                'reformulated_query': None,
                'is_greeting': True
            }
        
        # 1. Reformuler si n√©cessaire
        search_query = question
        if use_history and self.conversation_history:
            search_query = self.reformulate_query(question)
        
        # 2. Retrieval
        print("   üîç Recherche documents pertinents...")
        documents = self.retrieve_documents(query=search_query, n_results=n_results)
        print(f"   ‚úÖ {len(documents)} documents trouv√©s")
        
        # 3. Generate prompt
        print("   üìù Construction du prompt...")
        prompt = self.generate_prompt(
            question,
            documents, 
            include_history=use_history
        )
        
        # 4. Generate answer
        print("   ü§ñ G√©n√©ration de la r√©ponse...")
        answer = self.generate_answer(prompt, max_tokens=200)  # R√©duit pour concision
        
        # 5. Ajouter √† l'historique
        if use_history:
            self.add_to_history(question, answer)
        
        # 6. Format sources
        sources = [
            {
                'source': doc['metadata']['source'],
                'category': doc['metadata']['category'],
                'score': doc['score']
            }
            for doc in documents
        ]
        
        print("   ‚úÖ R√©ponse g√©n√©r√©e\n")
        
        return {
            'question': question,
            'answer': answer,
            'sources': sources,
            'reformulated_query': search_query if search_query != question else None,
            'is_greeting': False
        }

# ============================================
# FONCTION DE TEST
# ============================================

def test_conversation():
    """Teste une conversation multi-tours"""
    
    print("=" * 70)
    print("TEST CONVERSATION MULTI-TOURS")
    print("=" * 70 + "\n")
    
    rag = RAGService()
    
    # Conversation
    questions = [
        "Bonjour !"
        "Quand commence le semestre d'automne 2024 ?",
        "Et combien de temps dure-t-il ?",  # ‚Üê R√©f√©rence √† question pr√©c√©dente
        "Merci ! Maintenant, quelles sont les r√®gles d'absence ?"
    ]
    
    for i, q in enumerate(questions, 1):
        print(f"\n{'='*70}")
        print(f"TOUR {i}")
        print(f"{'='*70}")
        
        result = rag.ask(q, use_history=True)
        
        print(f"\n‚ùì {result['question']}")
        print(f"\nüí¨ {result['answer']}")
        print(f"\nüìö Sources : {', '.join([s['source'][:30] for s in result['sources']])}")
    
    # Effacer historique
    print(f"\n{'='*70}")
    rag.clear_history()


if __name__ == "__main__":
    # Test simple
    # test_rag_service()
    
    # Test conversation
    test_conversation()


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    test_conversation()
"""
Service RAG (Retrieval-Augmented Generation)

Architecture :
Question ‚Üí Embedding ‚Üí ChromaDB ‚Üí Top K chunks ‚Üí Prompt + LLM ‚Üí R√©ponse
"""

import os
from pathlib import Path
from typing import List, Dict, Optional

from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from huggingface_hub import InferenceClient
from dotenv import load_dotenv

load_dotenv()

# ============================================
# CONFIGURATION
# ============================================

CHROMA_DB_PATH = "data/chroma_db"
COLLECTION_NAME = "student_documents"
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
LLM_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")

# ============================================
# SERVICE RAG
# ============================================

class RAGService:
    """
    Service principal pour le RAG
    
    Responsabilit√©s :
    1. Recherche de documents pertinents (retrieval)
    2. G√©n√©ration de r√©ponse avec contexte (generation)
    3. Citation des sources
    """
    
    def __init__(self):
        """Initialise le service RAG"""
        
        print("üöÄ Initialisation du service RAG...")
        
        # 1. Mod√®le d'embeddings
        print(f"   üì¶ Chargement mod√®le embeddings...")
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
        
        # 2. ChromaDB
        print(f"   üóÑÔ∏è Connexion ChromaDB...")
        self.chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
        self.collection = self.chroma_client.get_collection(name=COLLECTION_NAME)
        
        # 3. LLM Client
        print(f"   ü§ñ Connexion HuggingFace API...")
        self.llm_client = InferenceClient(token=HUGGINGFACE_API_KEY)
        
        print("‚úÖ Service RAG pr√™t !\n")
    
    
    def retrieve_documents(
        self, 
        query: str, 
        n_results: int = 3,
        category_filter: Optional[str] = None
    ) -> List[Dict]:
        """
        Recherche les documents pertinents
        
        Args:
            query: Question de l'utilisateur
            n_results: Nombre de r√©sultats √† retourner
            category_filter: Filtrer par cat√©gorie (optionnel)
        
        Returns:
            Liste de documents avec m√©tadonn√©es
        """
        
        # G√©n√©rer embedding de la question
        query_embedding = self.embedding_model.encode([query])[0]
        
        # Rechercher dans ChromaDB
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=n_results,
            # where={"category": category_filter} if category_filter else None
        )
        
        # Formater les r√©sultats
        documents = []
        for doc, meta, distance in zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        ):
            documents.append({
                'text': doc,
                'metadata': meta,
                'score': 1 / (1 + abs(distance))  # Convertir distance en score 0-1
            })
        
        return documents
    
    
    def generate_prompt(
        self, 
        query: str, 
        documents: List[Dict]
    ) -> str:
        """
        Construit le prompt pour le LLM
        
        Structure :
        1. Instructions syst√®me
        2. Contexte (documents r√©cup√©r√©s)
        3. Question
        4. Instructions de r√©ponse
        """
        
        # Construire le contexte √† partir des documents
        context = ""
        for i, doc in enumerate(documents, 1):
            source = doc['metadata']['source']
            category = doc['metadata']['category']
            text = doc['text']
            
            context += f"\n[Document {i}]\n"
            context += f"Source: {source}\n"
            context += f"Cat√©gorie: {category}\n"
            context += f"Contenu: {text}\n"
            context += "-" * 60 + "\n"
        
        # Prompt complet
        prompt = f"""Tu es un assistant virtuel pour les √©tudiants de l'Universit√© Mohammed V de Rabat (UM5).
        Ton r√¥le :
        - R√©pondre aux questions sur les emplois du temps, r√®glements, proc√©dures et FAQ
        - Utiliser UNIQUEMENT les informations fournies dans le contexte
        - Citer les sources de tes informations
        - √ätre pr√©cis, bienveillant et professionnel
        Contexte disponible :
        {context}
        Question de l'√©tudiant : {query}
        Instructions pour ta r√©ponse :
        1. R√©ponds en te basant UNIQUEMENT sur le contexte ci-dessus
        2. Si l'information n'est pas dans le contexte, dis "Je n'ai pas cette information dans ma base de connaissances"
        3. Cite la source (nom du document) pour chaque information
        4. Sois concis mais complet
        5. Utilise un ton professionnel mais accessible
        6. Si les questions sont poses en Francais, tu Dois repondre en Francais et non en Anglais.
        7. Tu ne reponds en anglais que si on te pose des question en anglais sinon reponds en Francais
        R√©ponse :"""

        return prompt
    
    
    def generate_answer(
        self, 
        prompt: str, 
        max_tokens: int = 500,
        temperature: float = 0.7
    ) -> str:
        """
        G√©n√®re la r√©ponse avec le LLM
        
        Args:
            prompt: Prompt complet avec contexte
            max_tokens: Longueur max de la r√©ponse
            temperature: Cr√©ativit√© (0=d√©terministe, 1=cr√©atif)
        
        Returns:
            R√©ponse g√©n√©r√©e par le LLM
        """
        
        try:
            # Appel API HuggingFace
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            response = self.llm_client.chat_completion(
                messages=messages,
                model=LLM_MODEL,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            
            answer = response.choices[0].message.content
            return answer.strip()
            
        except Exception as e:
            return f"Erreur lors de la g√©n√©ration : {str(e)}"
    
    
    def ask(
        self, 
        question: str,
        n_results: int = 3,
        category_filter: Optional[str] = None
    ) -> Dict:
        """
        Pipeline RAG complet
        
        C'est la fonction principale qui orchestre tout :
        1. Retrieval (recherche)
        2. Prompt generation
        3. Answer generation
        4. Source formatting
        
        Args:
            question: Question de l'utilisateur
            n_results: Nombre de documents √† r√©cup√©rer
            category_filter: Filtrer par cat√©gorie
        
        Returns:
            Dict avec answer, sources, metadata
        """
        
        print(f"\n‚ùì Question : {question}")
        
        # 1. Retrieval
        print("   üîç Recherche documents pertinents...")
        documents = self.retrieve_documents(
            query=question,
            n_results=n_results,
            category_filter=category_filter
        )
        
        print(f"   ‚úÖ {len(documents)} documents trouv√©s")
        
        # 2. Generate prompt
        print("   üìù Construction du prompt...")
        prompt = self.generate_prompt(question, documents)
        
        # 3. Generate answer
        print("   ...G√©n√©ration de la r√©ponse...")
        answer = self.generate_answer(prompt)
        
        # 4. Format sources
        sources = [
            {
                'source': doc['metadata']['source'],
                'category': doc['metadata']['category'],
                'score': doc['score'],
                'excerpt': doc['text'][:200] + "..."
            }
            for doc in documents
        ]
        
        print("   ‚úÖ R√©ponse g√©n√©r√©e\n")
        
        return {
            'question': question,
            'answer': answer,
            'sources': sources,
            'metadata': {
                'n_documents_used': len(documents),
                'model': LLM_MODEL
            }
        }


# ============================================
# FONCTION DE TEST
# ============================================

def test_rag_service():
    """Teste le service RAG avec des questions exemples"""
    
    print("=" * 70)
    print("TEST DU SERVICE RAG")
    print("=" * 70 + "\n")
    
    # Initialiser le service
    rag = RAGService()
    
    # Questions test
    test_questions = [
        "Quand commence le semestre d'hiver 2024-2025 ?",
        "Quelles sont les r√®gles concernant les absences √† l'ENSIAS ?",
        "Comment s'inscrire √† l'UM5 pour 2025-2026 ?"
    ]
    
    # Tester chaque question
    for i, question in enumerate(test_questions, 1):
        print("=" * 70)
        print(f"TEST {i}/{len(test_questions)}")
        print("=" * 70)
        
        result = rag.ask(question)
        
        print(f"\nüìå QUESTION :")
        print(f"   {result['question']}")
        
        print(f"\nüí¨ R√âPONSE :")
        print(f"   {result['answer']}")
        
        print(f"\nüìö SOURCES ({len(result['sources'])}) :")
        for j, source in enumerate(result['sources'], 1):
            print(f"\n   {j}. {source['source']}")
            print(f"      Cat√©gorie : {source['category']}")
            print(f"      Score : {source['score']:.3f}")
            print(f"      Extrait : {source['excerpt'][:100]}...")
        
        print("\n")


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    test_rag_service()
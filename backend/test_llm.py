"""Test du LLM avec HuggingFace Inference API (GRATUIT) - CORRIGÃ‰"""

import os
from dotenv import load_dotenv
from huggingface_hub import InferenceClient

load_dotenv()

def test_llm():
    api_key = os.getenv("HUGGINGFACE_API_KEY")
    
    if not api_key:
        print("âŒ Token HuggingFace manquant dans .env")
        return False
    
    # ModÃ¨le Instruct â†’ mieux avec chat
    model = "mistralai/Mistral-7B-Instruct-v0.2"
    
    print(f"ğŸ¤– ModÃ¨le LLM : {model}")
    print("ğŸ”„ Test de gÃ©nÃ©ration de texte via chat...\n")
    
    try:
        client = InferenceClient(token=api_key)
        
        messages = [
            {"role": "user", "content": "Tu es un assistant pour Ã©tudiants. Quand commence le semestre d'hiver ? RÃ©ponds en une phrase courte."}
        ]
        
        print(f"ğŸ“ Prompt envoyÃ© :\n   {messages[0]['content']}\n")
        
        # Utilise chat_completion au lieu de text_generation
        response = client.chat_completion(
            messages=messages,
            model=model,
            max_tokens=100,
            temperature=0.7,
        )
        
        answer = response.choices[0].message.content
        print(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e :\n   {answer}\n")
        print("ğŸ‰ Test LLM rÃ©ussi avec chat_completion !")
        return True
        
    except Exception as e:
        print(f"âŒ ERREUR : {e}")
        if "rate limit" in str(e).lower():
            print("\nâ³ Rate limit atteint â†’ attends 1-2 min")
        return False

if __name__ == "__main__":
    print("=" * 50)
    print("TEST LLM HUGGINGFACE (GRATUIT) - FIXÃ‰")
    print("=" * 50 + "\n")
    test_llm()